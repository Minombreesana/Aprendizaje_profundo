{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9_1a5iymIvC"
      },
      "source": [
        "## 8. EJERCICIO\n",
        "\n",
        "Vamos a armar una pequeña competición en el curso.\n",
        "El objetivo es armar una arquitectura de CNN que identifique el dataset MNIST.\n",
        "Se van a usar capas de convolución, de activación y de pooling a elección. Cada alumno eligirá su modelo y los respectivos hiperparámetros, lo entrenará y presentará los siguientes resultados:\n",
        "\n",
        "*   `test_acc` (del test final)\n",
        "*   `n_parameter`\n",
        "*   `n_layers` (conv + activacion + pooling = 1 capa)\n",
        "*   `n_epochs` de entrenamiento usadas.\n",
        "\n",
        "\n",
        "El modelo se deberá ajustar a los siguientes parámetros:\n",
        "\n",
        "*   train: 80%, validation: 10%, test: 10% (los datos serán dados así todos usan el mismo set para cada grupo. Están en el github el curso).\n",
        "*   capa final de salida será una softmax de 10 elementos.\n",
        "*   coss_function será `CrossEntropyLoss`.\n",
        "\n",
        "El ganador de la competencia será aquel que consiga el mayor `score` empleando la siguietne fórmula:\n",
        "\n",
        "$$ score = \\frac{1}{log_{10}(n\\_parameter)} * \\frac{10}{n\\_epochs}*test\\_acc*n\\_layers$$\n",
        "\n",
        "Deberan presentar su código colab funcionando y el score alcanzado (con los valores de cada variable que compone el score).\n",
        "\n",
        "Es una competencia fairplay y con fines didácticos, esta formula del ```score``` fué inventada.... no usar como referencia para definir qué modelo utilizar.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wIQ8hjDpdVi"
      },
      "source": [
        "#### Importar lo necesario"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHQUjDs12DLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a53bff-2bd0-476c-8a0e-af91ff3afe9d"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchsummary\n",
        "!pip install torchmetrics\n",
        "import torchmetrics"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n",
            "\u001b[K     |████████████████████████████████| 419 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeJy8fjPn4wi"
      },
      "source": [
        "#### configuramos el `device` acorde al device disponible\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOV9xybtn4I3"
      },
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nQ-MLk6Do8e"
      },
      "source": [
        "1. Cargar base de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pickle.load( open( \"/content/train.pkl\", \"rb\" ) )\n",
        "test = pickle.load( open( \"/content/test.pkl\", \"rb\" ) )\n",
        "train_label = pickle.load( open( \"/content/train_label.pkl\", \"rb\" ) )\n",
        "test_label = pickle.load( open( \"/content/test_label.pkl\", \"rb\" ) )\n",
        "val = pickle.load( open( \"/content/val.pkl\", \"rb\" ) )\n",
        "val_label = pickle.load( open( \"/content/val_label.pkl\", \"rb\" ) )\n"
      ],
      "metadata": {
        "id": "HsDBdZFns6Ie"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.reshape(56000,1,28,28)\n",
        "test = test.reshape(7000, 1,28,28)\n",
        "val = val.reshape(7000,1, 28,28)"
      ],
      "metadata": {
        "id": "JJW8uWAX_xa8"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class YourDataset(Dataset):\n",
        "\n",
        "    def __init__(self, X_Train, Y_Train, transform=None):\n",
        "        self.X_Train = X_Train\n",
        "        self.Y_Train = Y_Train\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X_Train)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        x = self.X_Train[idx]\n",
        "        y = self.Y_Train[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "            y = self.transform(y)\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "_R6g6ESV2Aol"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = YourDataset(train, train_label)\n",
        "test_dataset = YourDataset(test, test_label)\n",
        "val_dataset = YourDataset(val, val_label)"
      ],
      "metadata": {
        "id": "SDpkpr4s2b2q"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikthAE4Dteb"
      },
      "source": [
        "2. Ver que la base de datos esté OK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyq2UFIl-Qjy"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "p2fs6Qdivs1H",
        "outputId": "3f285e99-4231-44d8-eae7-3629914d5aac"
      },
      "source": [
        "# Display image and label from dataloader (dataloader -> una herramienta para hacer batches de datasets)\n",
        "train_features, train_labels = next(iter(train_loader))\n",
        "\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "\n",
        "img = train_features[0]\n",
        "print('tamaño de 1 imagen: ', img.shape)\n",
        "# le QUITO 1 dimension (la del tamaño del batch) para poder graficar\n",
        "img = train_features[0].squeeze()\n",
        "print('tamaño 1 imagen DESPUES de squeeze: ', img.shape)\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n",
            "tamaño de 1 imagen:  torch.Size([1, 28, 28])\n",
            "tamaño 1 imagen DESPUES de squeeze:  torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0TN4erDxRd"
      },
      "source": [
        "3. Construyo mi CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding='same')\n",
        "        self.pool1 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding='same')\n",
        "        self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding='same')\n",
        "        self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv4 = torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding='same')\n",
        "        self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = torch.nn.Linear(in_features=128, out_features=100)\n",
        "        self.fc2 = torch.nn.Linear(in_features=100, out_features=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = self.pool3(torch.relu(self.conv3(x)))\n",
        "        x = self.pool4(torch.relu(self.conv4(x)))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "conv_model = ConvModel()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    conv_model.to(\"cuda\")\n",
        "\n",
        "torchsummary.summary(conv_model, (1, 28, 28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InK1JfTc7Ey8",
        "outputId": "3819cfd0-77a3-47b2-a188-252fb46cc268"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             160\n",
            "         MaxPool2d-2           [-1, 16, 14, 14]               0\n",
            "            Conv2d-3           [-1, 32, 14, 14]           4,640\n",
            "         MaxPool2d-4             [-1, 32, 7, 7]               0\n",
            "            Conv2d-5             [-1, 64, 7, 7]          18,496\n",
            "         MaxPool2d-6             [-1, 64, 3, 3]               0\n",
            "            Conv2d-7            [-1, 128, 3, 3]          73,856\n",
            "         MaxPool2d-8            [-1, 128, 1, 1]               0\n",
            "            Linear-9                  [-1, 100]          12,900\n",
            "           Linear-10                   [-1, 10]           1,010\n",
            "================================================================\n",
            "Total params: 111,062\n",
            "Trainable params: 111,062\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.22\n",
            "Params size (MB): 0.42\n",
            "Estimated Total Size (MB): 0.65\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model_optimizer = torch.optim.Adam(conv_model.parameters(), lr=0.001)\n",
        "\n",
        "conv_model_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "conv_model_accuracy = torchmetrics.Accuracy()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    conv_model_accuracy.to(\"cuda\")\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_train_loss = 0.0\n",
        "    epoch_train_accuracy = 0.0\n",
        "\n",
        "    for train_data, train_target in train_loader:\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            train_data = train_data.to(\"cuda\")\n",
        "            train_target = train_target.to(\"cuda\")\n",
        "\n",
        "        conv_model_optimizer.zero_grad()\n",
        "\n",
        "        train_data = train_data.float()\n",
        "    \n",
        "        output = conv_model(train_data)\n",
        "    \n",
        "        loss = conv_model_loss(output, train_target)\n",
        "        epoch_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "    \n",
        "        conv_model_optimizer.step()\n",
        "\n",
        "        accuracy = conv_model_accuracy(output, train_target)\n",
        "        epoch_train_accuracy += accuracy.item()\n",
        "\n",
        "  \n",
        "    epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "    epoch_train_accuracy = epoch_train_accuracy / len(train_loader)\n",
        "    train_loss.append(epoch_train_loss)\n",
        "    train_acc.append(epoch_train_accuracy)\n",
        "\n",
        " \n",
        "    epoch_valid_loss = 0.0\n",
        "    epoch_valid_accuracy = 0.0\n",
        "\n",
        "    for valid_data, valid_target in valid_loader:\n",
        "        if torch.cuda.is_available():\n",
        "            valid_data = valid_data.to(\"cuda\")\n",
        "            valid_target = valid_target.to(\"cuda\")\n",
        "\n",
        "        valid_data = valid_data.float()\n",
        " \n",
        "        output = conv_model(valid_data)\n",
        "        epoch_valid_loss += conv_model_loss(output, valid_target).item()\n",
        "        epoch_valid_accuracy += conv_model_accuracy(output, valid_target).item()\n",
        "      \n",
        "    epoch_valid_loss = epoch_valid_loss / len(valid_loader)\n",
        "    epoch_valid_accuracy = epoch_valid_accuracy / len(valid_loader)\n",
        "    valid_loss.append(epoch_valid_loss)\n",
        "    valid_acc.append(epoch_valid_accuracy)\n",
        "\n",
        "    print(\"Epoch: {}/{} - Train loss {:.6f} - Train Accuracy {:.6f} - Valid Loss {:.6f} - Valid Accuracy {:.6f}\".format(\n",
        "      epoch+1, epochs, epoch_train_loss, epoch_train_accuracy, epoch_valid_loss, epoch_valid_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gfjuGY0CvHr",
        "outputId": "5d4d0d38-d96e-4eed-eea6-f0b382c63da1"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20 - Train loss 0.062368 - Train Accuracy 0.981089 - Valid Loss 0.043384 - Valid Accuracy 0.986571\n",
            "Epoch: 2/20 - Train loss 0.043969 - Train Accuracy 0.986946 - Valid Loss 0.040650 - Valid Accuracy 0.988571\n",
            "Epoch: 3/20 - Train loss 0.033825 - Train Accuracy 0.989571 - Valid Loss 0.047982 - Valid Accuracy 0.987000\n",
            "Epoch: 4/20 - Train loss 0.027807 - Train Accuracy 0.991179 - Valid Loss 0.048544 - Valid Accuracy 0.986286\n",
            "Epoch: 5/20 - Train loss 0.025973 - Train Accuracy 0.992000 - Valid Loss 0.036297 - Valid Accuracy 0.990286\n",
            "Epoch: 6/20 - Train loss 0.023037 - Train Accuracy 0.992839 - Valid Loss 0.046014 - Valid Accuracy 0.988571\n",
            "Epoch: 7/20 - Train loss 0.019077 - Train Accuracy 0.993982 - Valid Loss 0.045144 - Valid Accuracy 0.988714\n",
            "Epoch: 8/20 - Train loss 0.019659 - Train Accuracy 0.994232 - Valid Loss 0.038552 - Valid Accuracy 0.990857\n",
            "Epoch: 9/20 - Train loss 0.016979 - Train Accuracy 0.994875 - Valid Loss 0.039853 - Valid Accuracy 0.990714\n",
            "Epoch: 10/20 - Train loss 0.017230 - Train Accuracy 0.995161 - Valid Loss 0.050696 - Valid Accuracy 0.988429\n",
            "Epoch: 11/20 - Train loss 0.013038 - Train Accuracy 0.996071 - Valid Loss 0.049573 - Valid Accuracy 0.988429\n",
            "Epoch: 12/20 - Train loss 0.014477 - Train Accuracy 0.996054 - Valid Loss 0.051087 - Valid Accuracy 0.990286\n",
            "Epoch: 13/20 - Train loss 0.013012 - Train Accuracy 0.996071 - Valid Loss 0.066679 - Valid Accuracy 0.986571\n",
            "Epoch: 14/20 - Train loss 0.013996 - Train Accuracy 0.995964 - Valid Loss 0.045394 - Valid Accuracy 0.991429\n",
            "Epoch: 15/20 - Train loss 0.013276 - Train Accuracy 0.996679 - Valid Loss 0.051202 - Valid Accuracy 0.989286\n",
            "Epoch: 16/20 - Train loss 0.012793 - Train Accuracy 0.996375 - Valid Loss 0.082339 - Valid Accuracy 0.985714\n",
            "Epoch: 17/20 - Train loss 0.014273 - Train Accuracy 0.995911 - Valid Loss 0.065144 - Valid Accuracy 0.988429\n",
            "Epoch: 18/20 - Train loss 0.013562 - Train Accuracy 0.995857 - Valid Loss 0.076473 - Valid Accuracy 0.987000\n",
            "Epoch: 19/20 - Train loss 0.010724 - Train Accuracy 0.997250 - Valid Loss 0.063107 - Valid Accuracy 0.987571\n",
            "Epoch: 20/20 - Train loss 0.010623 - Train Accuracy 0.997107 - Valid Loss 0.054122 - Valid Accuracy 0.988286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "test_accuracy = 0.0\n",
        "\n",
        "for test_data, test_target in test_loader:\n",
        "    if torch.cuda.is_available():\n",
        "        test_data, test_target = test_data.cuda(), test_target.cuda()\n",
        "\n",
        "    test_data = test_data.float()   \n",
        "    output = conv_model(test_data)\n",
        "    test_loss += conv_model_loss(output, test_target).item()\n",
        "    test_accuracy += conv_model_accuracy(output, test_target).item()\n",
        "\n",
        "test_loss = test_loss / len(test_loader)\n",
        "test_accuracy = test_accuracy / len(test_loader)\n",
        "\n",
        "print(\"El modelo logro un error de {:.6f} y una accuracy de {:.6f}\".format(test_loss, test_accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UwHT4Y0EsPJ",
        "outputId": "67e78a5c-ecbb-493e-d90b-33a99a7db860"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo logro un error de 0.056514 y una accuracy de 0.990571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# corremos 1 dato, a ver como lo clasifica...\n",
        "# generamos un batch del dataloader\n",
        "test_features, test_labels = next(iter(test_loader\n",
        "                                       ))\n",
        "\n",
        "# item a usar k\n",
        "k = 50\n",
        "\n",
        "# verifico las dimensiones y los valores que toma algun pixel.\n",
        "samp_img = test_features[k]\n",
        "print(samp_img.shape)\n",
        "print(samp_img[0][0][0])\n",
        "print(torch.max(samp_img))\n",
        "print(torch.min(samp_img))\n",
        "# ploteo la imagen\n",
        "plt.imshow(samp_img.squeeze(), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "# preparo para pasarla a la red (model) asi predice.\n",
        "samp_imp = samp_img.unsqueeze(0) # agrego la batch dim\n",
        "samp_img = samp_img.unsqueeze(0).to(device)\n",
        "samp_img = samp_img.float()\n",
        "print(samp_img.shape)\n",
        "\n",
        "# la paso al modelo\n",
        "conv_model.eval()\n",
        "y_hat = conv_model(samp_img)\n",
        "print('Predición del modelo:')\n",
        "print(y_hat)\n",
        "print('softmax de predicción:')\n",
        "print(torch.nn.functional.softmax(y_hat, dim=1))\n",
        "print(f'El numero es un: ', torch.argmax(y_hat, axis=1).item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "madQMUdLGR5C",
        "outputId": "36ef01c4-2bd5-4479-f979-32659c0268a3"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "tensor(0, dtype=torch.uint8)\n",
            "tensor(255, dtype=torch.uint8)\n",
            "tensor(0, dtype=torch.uint8)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 28, 28])\n",
            "Predición del modelo:\n",
            "tensor([[-19.0334,  -3.7215,  25.0505,  -9.0848,  -8.3753, -30.6435, -10.6920,\n",
            "          -7.9584,  -6.8816, -24.1721]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "softmax de predicción:\n",
            "tensor([[7.1544e-20, 3.1948e-13, 1.0000e+00, 1.4969e-15, 3.0433e-15, 6.4920e-25,\n",
            "         3.0006e-16, 4.6175e-15, 1.3553e-14, 4.1966e-22]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "El numero es un:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# score final"
      ],
      "metadata": {
        "id": "1aQ5n86Kwk7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = test_accuracy\n",
        "n_params = 111.062\n",
        "n_layers = 6\n",
        "n_epochs = 20\n",
        "\n",
        "score = 1/np.log(n_params) * 10/n_epochs * test_acc * n_layers\n",
        "print('el Score final es:', score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHEpkN6NFBuY",
        "outputId": "49337e51-c1a7-4f91-9fd0-6a7f6d731703"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el Score final es: 0.6309253535731462\n"
          ]
        }
      ]
    }
  ]
}